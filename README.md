# ETL_Project
ETL_Project Report by Kosal Sieng, Sean Istre, and Carter Alvarez

For our ETL project we took data from the EIA website to look at electricity data. We gathered three data sets to get good variation for our final database. Our three data sets were the supply, demand, and a pricing information about electricity usage in the United States across 3 sectors: residential, commercial, and industrial. The demand and pricing data sets were both csv files, and the supply file was a JSON. All three of our data sets were all set during the same 20 year time frame, with new data points coming each month. Due to this, we decided to create a final sequel-alchemy data base that shared the same primary key for date_id, so that we could run queries to find electricity data quickly from the different quarters, seasons, months etc across all of our data. 


## PART 1. Electricity Price and Demand Data
First, we took the demand and price data sets and started to import them into our jupyter notebook so we could clean them using Python/Pandas. We did this by reading the CSV files we downloaded into the notebook using pandas. The price and demand data sets were similar in how they were built, so we created code that cleaned up both of them seamlessly. We deleted the columns and rows that we did not need, as there was about 6 rows of heading that was useless for the data base, and 3 rows of extra columns that restated identifications. We then ran a line of code that filtered the data set so that it was in descending order based on date, to match our external date_id excel sheet, and then renamed our index column as date_id for the final connection in sequal. Once the data frames were cleaned up, we started to create our code to export these data frames into our sequel data base.  


## PART 2. Electricity Supply Data
Energy sources were downloaded in a ".json" file. The JSON files content numerous datas that needed to be cleaned up. First, JSON file was loaded to the jupyter notebook using "json.load". After inspecting the strings, the data is separated into 12 data sets that content information about each data source. "s.find" and "s.rfind" are used to extract the name of the energy source from the string. Two "for loop" are used to create dataframes for each source and joint all energy sources into one big dataframe. Then, we need to clean up dataframes, by rename, reset index and set index key that will be transfered to the postgres database.


## PART 3. Date Data
In examining the three data sets we obtained, the date format left much to be desired, being a string concatination of the of the year and the month, for example "200101" refers to January of 2001. Since the date was the common data that linked all three tables, and all three data sets were compiled monthly with the same date range, we decided to pull the date information from the individual tables and consolidate it in a new table called "Date" reducing the amount of repeated data and adding more functionality. The three data sets with the dates replaced with a "date_id" could now use this new table to combine data using joins. Due to the time constraints of the project the data for the "Date" table was created using Excel. In, addition to the year and month data, now separated into separate columns, quarter and season columns were created using nested Excel IF and OR functions to speed up the data entry rather than manually entering all the values. Season were abbreviated as follows: Winter - WIN, Spring - SPR, Summer - SUM, and Autumn - AUT. From there a CSV file was created. The CSV was then read into a jupyter notebook using Pandas and using sqlalchemy, loaded into the database.

## PART 4. Constructing the Postgres SQL Database
Now that we had all of the data cleaned and ready to go, it case time to store the data for analysis using a Postgres SQL Database. Using quickdatabasediagrams.com, we created a ERD diagram of our database tables and their relationships. Since we removed the significant header rows from all the data sets, we decided to name the tables what the data was and what units it was in, so that anyone using the data would know what the data was without having to ask or do any research. An image of the diagram (ETL_Project_ERD.png) can be found in the Resources folder of the repository. From this diagram, we created a SQL Schema file (ETL_Project_schema.sql) also in the Resources folder to use in PGAdmin to create the tables of our database. Once the tables were created, we began loading the data from the jupyter notebook Pandas dataframe directly into the database tables using sqlalchemy, being sure to load the "Date" table first as it had the master "date_id" values. After loading all of the data into the database, we did a sanity check by doing a "SELECT *" query from all four tables finding all of the data was successfully added to the database.



Source: https://www.eia.gov/beta/states/data
